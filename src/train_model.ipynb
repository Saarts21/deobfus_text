{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from resnet18 import ResNet18\n",
    "from letters import *\n",
    "from sparse_attack import l0_pgd_attack\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"using Cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_built():\n",
    "    print(\"using MPS\")\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    print(\"using CPU\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"robust\"\n",
    "#model = ResNet18(num_classes=ABC_SIZE + len(additional_symbols))\n",
    "model = ResNet18()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and data loader\n",
    "csv_file = f\"data/letters_{experiment_name}.csv\"\n",
    "dataset = LettersDataset(csv_file, device)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Compute the mean and std of the entire dataset\n",
    "mean = torch.mean(torch.stack([array.mean() for array, _ in dataset]))\n",
    "std = torch.std(torch.stack([array.std() for array, _ in dataset]))\n",
    "print(f\"{mean = }\")\n",
    "print(f\"{std = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "\n",
    "\"\"\"\n",
    "Instead of using one-hot labels (e.g., \"L\" = [0, 1, 0, â€¦]), apply label smoothing.\n",
    "For example, if there are 26 classes, instead of assigning a probability of 1.0 to the correct class,\n",
    "assign 0.9 and spread the remaining 0.1 across all other classes equally.\n",
    "This prevents the model from becoming overconfident.\n",
    "\"\"\"\n",
    "criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for arrays, labels in tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        arrays, labels = arrays.to(device), labels.to(device)\n",
    "        arrays = (arrays - mean) / std\n",
    "\n",
    "        # === With 50% probability, use adversarial examples ===\n",
    "        if random.random() < 0.5:\n",
    "            delta = l0_pgd_attack(arrays, labels, model, k=10, alpha=0.1, steps=10)\n",
    "            arrays = (arrays + delta).clamp(-1, 1)\n",
    "\n",
    "        # Training step\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(arrays)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(data_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model .pth\n",
    "torch.save(model.state_dict(), f'models/resnet18_{experiment_name}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
