{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detect_adversarial import *\n",
    "from splitter import *\n",
    "from viper import *\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# target model\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# substitute model\n",
    "from substitute import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load target model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/dehatebert-mono-english\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/english-abusive-MuRIL\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/english-abusive-MuRIL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subst_model = load_substitution_model(\"robust\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deobfuscate_adversarial_example(adv):\n",
    "    clean_words = []\n",
    "    split_words = split_text_to_strings(adv)\n",
    "\n",
    "    for word in tqdm(split_words, desc=\"Deobfuscating words\"):\n",
    "        if is_valid_word_light(word):\n",
    "            clean_words.append(word)\n",
    "        else:\n",
    "            all_splits = split_string_to_substrings(word)\n",
    "            \n",
    "            substitutions = []\n",
    "            scores = []\n",
    "            max_substitution = word\n",
    "            max_score = float('-inf')\n",
    "            for split in all_splits:\n",
    "                options = substitute_split(split, subst_model)\n",
    "\n",
    "                if options:\n",
    "                    for substitution, score in options:\n",
    "                        if score > max_score:\n",
    "                            max_substitution = substitution\n",
    "                            max_score = score\n",
    "                        if not is_word_exists(substitution):\n",
    "                            continue\n",
    "                        substitutions.append(substitution)\n",
    "                        scores.append(score)\n",
    "\n",
    "            if not substitutions:\n",
    "                # fail to find a valid clean word - get the most likely which is not necessarily valid\n",
    "                clean_words.append(max_substitution)\n",
    "                continue\n",
    "            \n",
    "            # pick the substitution with the maxumim score among all valid words\n",
    "            likely_substitution = substitutions[scores.index(max(scores))]\n",
    "            clean_words.append(likely_substitution)\n",
    "    \n",
    "    # attach the strings in the list clean_words to one string\n",
    "    clean_text = ''.join(clean_words)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/HateSpeechDatasetBalanced_filtered.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random example from the DataFrame\n",
    "random_example = df.sample(n=1)\n",
    "content = random_example[\"Content\"].values[0]\n",
    "label = random_example[\"Label\"].values[0]\n",
    "print(content)\n",
    "\n",
    "adversarial = viper(content, 0.1)\n",
    "print(adversarial)\n",
    "\n",
    "clean = deobfuscate_adversarial_example(adversarial)\n",
    "print(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_hateful(text):\n",
    "    input = tokenizer(text, return_tensors=\"pt\")\n",
    "    output = model(**input)\n",
    "    classification = output.logits.argmax(dim=1).item()\n",
    "    return classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full pipeline\n",
    "def full_pipline(p, examples):\n",
    "    file_path = \"data/HateSpeechDatasetBalanced_filtered.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    model_orig_accuracy = 0\n",
    "    model_adversarial_accuracy = 0\n",
    "    model_clean_accuracy = 0\n",
    "\n",
    "    n_examples = 0\n",
    "    avrg_orig_adv_dist = 0\n",
    "    avrg_orig_clean_dist = 0\n",
    "    adversarials = 0\n",
    "    success = 0\n",
    "    for i, row in df.iterrows():\n",
    "        if i in [16, 38]:\n",
    "            continue\n",
    "        if n_examples == examples:\n",
    "            break\n",
    "        print(i)\n",
    "        n_examples += 1\n",
    "        orig = row[\"Content\"]\n",
    "        adv = viper(orig, p)\n",
    "        orig_adv_dist = edit_distance(orig, adv)\n",
    "        avrg_orig_adv_dist += orig_adv_dist\n",
    "\n",
    "        orig_pred = predict_hateful(orig)\n",
    "        model_orig_accuracy += orig_pred\n",
    "        adv_pred = predict_hateful(adv)\n",
    "        model_adversarial_accuracy += adv_pred\n",
    "\n",
    "        clean = deobfuscate_adversarial_example(adv)\n",
    "        orig_clean_dist = edit_distance(orig, clean)\n",
    "        avrg_orig_clean_dist += orig_clean_dist\n",
    "        clean_pred = predict_hateful(clean)\n",
    "        model_clean_accuracy += clean_pred\n",
    "\n",
    "        if orig_pred == 1 and \\\n",
    "            adv_pred == 0:\n",
    "            adversarials += 1\n",
    "            if clean_pred == 1:\n",
    "                success += 1\n",
    "\n",
    "                print(f\"Original:    {orig}\")\n",
    "                print(f\"Adversarial: {adv}\")\n",
    "                print(f\"Clean:       {clean}\")\n",
    "\n",
    "    avrg_orig_adv_dist /= n_examples\n",
    "    avrg_orig_clean_dist /= n_examples\n",
    "    cleaning_average_distance = avrg_orig_adv_dist - avrg_orig_clean_dist\n",
    "\n",
    "    print(f\"Average original-adversarial edit distance: {avrg_orig_adv_dist}\")\n",
    "    print(f\"Average original-clean edit distance: {avrg_orig_clean_dist}\")\n",
    "    print(f\"Cleaning average distance: {cleaning_average_distance}\")\n",
    "    \n",
    "    print(f\"Adversarial examples generated: {adversarials}\")\n",
    "    print(f\"Successful deobfuscations: {success}\")\n",
    "\n",
    "    print(f\"Model original accuracy: {model_orig_accuracy / n_examples}\")\n",
    "    print(f\"Model adversarial accuracy: {model_adversarial_accuracy / n_examples}\")\n",
    "    print(f\"Model clean accuracy: {model_clean_accuracy / n_examples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipline(0.2, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
