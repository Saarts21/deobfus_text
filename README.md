# Defending Hate Speech Detection against Visual Adversarial Attacks

This project addresses the vulnerability of hate speech detection systems to character-level adversarial attacks, particularly "leetspeak" substitutions that replace standard characters with visually similar symbols. While these obfuscation techniques can significantly degrade model performance, existing defenses often require expensive retraining or fail to generalize across attack variations. I propose a novel model-agnostic preprocessing defense that can be integrated with any pre-trained hate speech detection system without modification. My approach employs a ResNet-18 vision classifier trained on corrupted character images to reconstruct obfuscated words into their original form. The classifier, augmented with corruptions and adversarial training with sparse attacks, achieves 51\% accuracy on character recognition. When evaluated on two target models, my deobfuscation procedure successfully restored approximately 50\% of adversarially attacked samples and reduced performance degradation by half compared to undefended models. The method recovered about 30\% of altered characters while maintaining the semantic meaning required for correct classification. Unlike previous approaches that rely on pixel similarity or require costly adversarial retraining of the entire model, my defense provides an efficient and generalizable solution that can be applied to existing systems with minimal overhead.
